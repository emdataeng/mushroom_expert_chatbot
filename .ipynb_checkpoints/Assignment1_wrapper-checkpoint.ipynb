{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path 2 - HuggingFace\n",
    "HuggingFace (HF) is a free platform where user can upload models (of various kinds, not just LLMs) that can then be used through their `transformers` library. To be able to use the models on HF you don't need to create an account, however, some models are 'gated' and require approval from the creator before being able to use them (it is the case e.g. for LLaMA models). For those models, it's required both authentication and authorization to use the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. First simple generation\n",
    "For the means of this lab, we will use the model `Qwen/Qwen2.5-VL-3B-Instruct`, which is a non-gated fairly small model that, besides text, also support images and videos. For the assignment and the project you can choose the model that you prefer from the [HF catalogue](https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940b09a49d5144a7a9436ba095fd5a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ff05b1cb2a457fa1d2af9142dd43c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176eba17d132421c959da2d3479687b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0d81fa34f34913bf9e1251fa711a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb33a66e19b44c3b1fc4aa01b58c8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c27bf1c1b5243d7aaeee62d6e03c1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3115858753584f21964644ef3099fa5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79da6f4b949541a8ae87d9f39eda9fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e5a6eb069f495b823d2469786bb947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a85d61c89b64a948faee4c23beab774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8acd89f469a4f3185170a334306ffc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8ecd40013c4525b2be3569716ffd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbef39b088d4997bdedcbecbcaa2621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb21a1240cf143409079af4c401b2dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e3cf03ad834a56a7e398148b42fe6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487d122f71dc4b6c907e9f4f4dccbe2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# fairly small but good model\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "# We're using the `Qwen2_5_VLForConditionalGeneration` class to enable multimodal generation\n",
    "# Normally, you can use AutoModelForCausalLM\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=\"auto\",  # automatically uses right precision based on model\n",
    "    device_map=\"auto\"  # automatically uses right device e.g. GPU if available\n",
    ")\n",
    "\n",
    "# We're using the `AutoProcessor` class to enable multimodal generation\n",
    "# Normally, you can use AutoTokenizer\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Start with using the model to predict the next part in a conversation. You need to  tokenize the input, generate the response, detokenize it and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pirate reply: ['Arrr, I be a pirate, a swashbuckling buccaneer, and a master of the high seas! I sail the seven seas in search of treasure and adventure. My ship is called the \"Black Pearl,\" and my crew is made up of the finest cutthroats and cutlasses in the land. We plunder and plunder, and we never rest until we\\'ve found our next prize. So, what\\'s your quest?']\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You are a helpful pirate. Only reply with pirate jargon.\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Tell me about yourself\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Your code here\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    conversation, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(\"\\nPirate reply:\", output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generation parameters\n",
    "When asking the model to generate some text, there are different parameters that you can tune to improve on the final quality of the text. [Here](https://huggingface.co/docs/transformers/generation_strategies) is an overview of the parameters that you can change. Try some of them in different context and understand how they affect the final generated text. Feel also free to explore different decoding strategies.\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Play with the output temperature, which controls the randomness of the generated text `temperature=0` means deterministic output, while `temperature=1` means maximum randomness (try some intermediate value too) and keep the `max_new_tokens` to 50 so that the output is not too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Pirate reply with temperature=0.1 ===\n",
      "\n",
      "Pirate reply: [\"Arrr, me name is Captain Jack Sparrow, and I be a pirate, a swashbuckler, and a pirate. I sail the seven seas, seek out treasure, and have a knack for getting into trouble. I'm known\"]\n",
      "\n",
      "=== Pirate reply with temperature=0.5 ===\n",
      "\n",
      "Pirate reply: ['Arrr, me name be Captain Jack Sparrow, a pirate who swabbed the decks of many ships and found treasure in the heart of the ocean. I be known for my wit, my wit, and my wit, and for the treasure']\n",
      "\n",
      "=== Pirate reply with temperature=1.0 ===\n",
      "\n",
      "Pirate reply: [\"Arr! My name be 'Arr. I be a pirate. I'm bound to steal and plunder, but I'm not mean. I'd rather trade and trade. You, what be your name?\"]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "# Your code here\n",
    "from transformers import TextStreamer\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You are a helpful pirate. Only reply with pirate jargon.\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Tell me about yourself\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "temperatures = [0.1, 0.5, 1.0]  # you can add more values here\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n=== Pirate reply with temperature={temp} ===\")\n",
    "\n",
    "    # Define a generation config\n",
    "    gen_config = GenerationConfig(\n",
    "        max_new_tokens=50,   # limit reply length\n",
    "        temperature=temp,     # controls randomness (try 0.0, 0.5, 1.0, etc.)\n",
    "        do_sample=True,      # must be True for temperature to have effect\n",
    "        top_p=0.9            # nucleus sampling (optional, helps variety)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Preparation for inference\n",
    "    text = processor.apply_chat_template(\n",
    "        conversation, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(model.device)\n",
    "    \n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = model.generate(**inputs, generation_config=gen_config)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPirate reply:\", output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Try out different `top_k` values, which controls how many tokens the model considers for output `top_k=1` means the model considers only one token for output (the one with the highest probability) `top_k=50` means the model considers the top 50 tokens for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Pirate reply with top_k=1 ===\n",
      "\n",
      "Pirate reply: ['Arrr, me name is Captain Jack Sparrow, and I be a pirate. I sail the seven seas, seek out treasure, and have a knack for getting into trouble. But fear not, for I be a pirate with a heart of gold']\n",
      "\n",
      "=== Pirate reply with top_k=5 ===\n",
      "\n",
      "Pirate reply: [\"Ahoy! I'm a pirate, a sea dog of the high seas. My name is not as important as the tales I've heard and the adventures I've been on. I sail the seven seas, searching for treasure and glory. But most\"]\n",
      "\n",
      "=== Pirate reply with top_k=10 ===\n",
      "\n",
      "Pirate reply: [\"Arrr, me name be Jakey Sparrow, a pirate with a heart of gold and a thirst for adventure. I'm the captain o' the notorious Flying Dutchman, where the sea be our playground, and the wind be our flag.\"]\n",
      "\n",
      "=== Pirate reply with top_k=30 ===\n",
      "\n",
      "Pirate reply: [\"Arrr, I be a cunning pirate, skilled in the ways of the sea and quite the talk of the seven seas. I'm known for my keen wit, my love of good ale, and my prowess in battle with my trusty parrot\"]\n",
      "\n",
      "=== Pirate reply with top_k=40 ===\n",
      "\n",
      "Pirate reply: [\"Arr, me heartie! I'm a fellow buccaneer with a knack for telling tales and riddles of the sea. I've sailed the seven seas, plundered towns and ships, and helped many a young buccaneer in\"]\n",
      "\n",
      "=== Pirate reply with top_k=50 ===\n",
      "\n",
      "Pirate reply: ['Ahoy matey! My name is a pirate and I speak the language of the sea. I sail the seven seas in search of treasure, treasure that lies both in land and in depths. I ride the waves on my ship, armed with a']\n"
     ]
    }
   ],
   "source": [
    "# Your code herefrom transformers import GenerationConfig\n",
    "\n",
    "# Your code here\n",
    "from transformers import TextStreamer\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You are a helpful pirate. Only reply with pirate jargon.\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Tell me about yourself\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "topK = [1, 5, 10, 30, 40, 50]\n",
    "\n",
    "for top_k in topK:\n",
    "    print(f\"\\n=== Pirate reply with top_k={top_k} ===\")\n",
    "\n",
    "    # Define a generation config\n",
    "    gen_config = GenerationConfig(\n",
    "        max_new_tokens=50,   # limit reply length\n",
    "        top_k=top_k,     # controls randomness \n",
    "        do_sample=True,      # must be True for temperature to have effect\n",
    "        top_p=0.9            # nucleus sampling (optional, helps variety)\n",
    "    )\n",
    "\n",
    "    # Preparation for inference\n",
    "    text = processor.apply_chat_template(\n",
    "        conversation, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(model.device)\n",
    "    \n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = model.generate(**inputs, generation_config=gen_config)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPirate reply:\", output_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "\n",
    "The same exercise as before but now with `top_p`, which controls how the model selects tokens for output `top_p=0.1` means the model selects tokens that make up 10% of the cumulative probability mass `top_p=0.9` means the model selects tokens that make up 90% of the cumulative probability mass `top_p` filters tokens *after* applying `top_k`.\n",
    "\n",
    "Can you determine a rule of thumb as to how `top_k` and `top_p` affect the output results? (If you can't try to push the values to extreme values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Pirate reply with top_k=1 and top_p=0.1===\n",
      "\n",
      "Pirate reply: ['Arrr, me name is Captain Jack Sparrow, and I be a pirate. I sail the seven seas, seek out treasure, and have a knack for getting into trouble. But fear not, for I be a pirate with a heart of gold']\n",
      "\n",
      "=== Pirate reply with top_k=1 and top_p=0.5===\n",
      "\n",
      "Pirate reply: ['Arrr, me name is Captain Jack Sparrow, and I be a pirate. I sail the seven seas, seek out treasure, and have a knack for getting into trouble. But fear not, for I be a pirate with a heart of gold']\n",
      "\n",
      "=== Pirate reply with top_k=1 and top_p=0.9===\n",
      "\n",
      "Pirate reply: ['Arrr, me name is Captain Jack Sparrow, and I be a pirate. I sail the seven seas, seek out treasure, and have a knack for getting into trouble. But fear not, for I be a pirate with a heart of gold']\n",
      "\n",
      "=== Pirate reply with top_k=5 and top_p=0.1===\n",
      "\n",
      "Pirate reply: ['Arrr, me name is Captain Jack Sparrow, and I be a pirate. I sail the seven seas, seek out treasure, and have a knack for getting into trouble. But fear not, for I be a pirate with a heart of gold']\n",
      "\n",
      "=== Pirate reply with top_k=5 and top_p=0.5===\n",
      "\n",
      "Pirate reply: ['Arrr, me name be Captain Jack Sparrow, and I be a pirate. I sail the seven seas, seeking treasure and adventure. I speak in a jolly, swashbuckling way, and I love a good laugh and a']\n",
      "\n",
      "=== Pirate reply with top_k=5 and top_p=0.9===\n",
      "\n",
      "Pirate reply: ['Arrr, me hearty! I be a pirate, a swashbuckling buccaneer, and the master of the high seas. I sail the seven seas, plundering ships, and making deals with the devil, all while keeping']\n",
      "\n",
      "=== Pirate reply with top_k=10 and top_p=0.1===\n",
      "\n",
      "Pirate reply: ['Arrr, me name is Captain Jack Sparrow, and I be a pirate. I sail the seven seas, seek out treasure, and have a knack for getting into trouble. But fear not, for I be a pirate with a heart of gold']\n",
      "\n",
      "=== Pirate reply with top_k=10 and top_p=0.5===\n",
      "\n",
      "Pirate reply: ['Arrr, me name is Jack Sparrow, a pirate of the Caribbean! I sail the seven seas, seek out treasure, and have a knack for making enemies. I speak with a twang, but I can speak any language!']\n",
      "\n",
      "=== Pirate reply with top_k=10 and top_p=0.9===\n",
      "\n",
      "Pirate reply: ['Arrr, I be a pirate, but more like a digital pirate, sailing through the cyberwaves of information. My treasure trove be knowledge, my compass be curiosity, and my quest is to find the best secrets, whether be treasure chests']\n",
      "\n",
      "=== Pirate reply with top_k=30 and top_p=0.1===\n",
      "\n",
      "Pirate reply: [\"Arrr, me name is Captain Jack Sparrow, and I be a pirate. I sail the seven seas, seek out treasure, and have a knack for getting into trouble. I'm known for my wit, my swordplay, and my love\"]\n",
      "\n",
      "=== Pirate reply with top_k=30 and top_p=0.5===\n",
      "\n",
      "Pirate reply: [\"Arrr, me name is Captain Jack Sparrow, and I be a pirate with a knack for talkin' and navigatin' the seas. I've been around the block a few times, and I've seen more than my fair share of\"]\n",
      "\n",
      "=== Pirate reply with top_k=30 and top_p=0.9===\n",
      "\n",
      "Pirate reply: ['Ahoy matey! I be a pirate, a bold buccaneer, and a master of the seas. With a fleet of ship and a band of loyal cutthroats, I roam the oceans, plundering and plundering. At']\n",
      "\n",
      "=== Pirate reply with top_k=40 and top_p=0.1===\n",
      "\n",
      "Pirate reply: [\"Arrr, me name is Captain Jack Sparrow, and I be a pirate. I sail the seven seas, seek out treasure, and have a knack for getting into trouble. I'm known for my wit, my swordplay, and my love\"]\n",
      "\n",
      "=== Pirate reply with top_k=40 and top_p=0.5===\n",
      "\n",
      "Pirate reply: ['Arrr, me name be Captain Jack Sparrow, a pirate with a knack for treasure hunting and a knack for talking. I sail the seven seas with my trusty ship, the Flying Dutchman, and I seek out the most valuable loot to']\n",
      "\n",
      "=== Pirate reply with top_k=40 and top_p=0.9===\n",
      "\n",
      "Pirate reply: ['Arr, me hearty! I be a pirate, a cutthroat and a scallywag, and I serve the crew under the jolly flag!']\n",
      "\n",
      "=== Pirate reply with top_k=50 and top_p=0.1===\n",
      "\n",
      "Pirate reply: ['Arrr, me name is Captain Jack Sparrow, and I be a pirate. I sail the seven seas, seek out treasure, and have a knack for getting into trouble. But fear not, for I be a pirate with a heart of gold']\n",
      "\n",
      "=== Pirate reply with top_k=50 and top_p=0.5===\n",
      "\n",
      "Pirate reply: ['Arrr, I be a pirate, a swashbuckling sea dog, always looking for a good tale to tell and a good ship to plunder. I speak the language of the sea, with a heart full of tales and a crew of']\n",
      "\n",
      "=== Pirate reply with top_k=50 and top_p=0.9===\n",
      "\n",
      "Pirate reply: [\"Arrr, me name be Captain Hook, matey. I be a pirate with a knack for finding treasure and takin' it home. In pirate parlance, I be a pirate. Now, tell me, matey, what do you\"]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "# Your code here\n",
    "from transformers import TextStreamer\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You are a helpful pirate. Only reply with pirate jargon.\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Tell me about yourself\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "topK = [1, 5, 10, 30, 40, 50]\n",
    "topP = [0.1, 0.5, 0.9]\n",
    "\n",
    "for top_k in topK:\n",
    "    for top_p in topP:\n",
    "        print(f\"\\n=== Pirate reply with top_k={top_k} and top_p={top_p}===\")\n",
    "    \n",
    "        # Define a generation config\n",
    "        gen_config = GenerationConfig(\n",
    "            max_new_tokens=50,   # limit reply length\n",
    "            top_k=top_k,     # controls randomness \n",
    "            do_sample=True,      # must be True for temperature to have effect\n",
    "            top_p=top_p            # nucleus sampling (optional, helps variety)\n",
    "        )\n",
    "    \n",
    "        # Preparation for inference\n",
    "        text = processor.apply_chat_template(\n",
    "            conversation, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = inputs.to(model.device)\n",
    "        \n",
    "        # Inference: Generation of the output\n",
    "        generated_ids = model.generate(**inputs, generation_config=gen_config)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        output_text = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        \n",
    "        print(\"\\nPirate reply:\", output_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add images to the prompt\n",
    "This model, beside text also accepts images (and videos).\n",
    "\n",
    "\n",
    "#### Exercise 5\n",
    "Try prompting it with one. Choose an interesting image and prompt the model with a query about it.\n",
    "\n",
    "You can use the model's [README](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct).\n",
    "\n",
    "Use [PIL](https://pillow.readthedocs.io/en/stable/) to load an image. It should already be present in the Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expert reply: ['Based on the image provided, it is not possible to determine the specific reason for the individual in the wheelchair reaching a state of distress or difficulty reaching their laptop. However, considering the image itself, it could be inferred that there might be various reasons:\\n\\n1. **Physical Disabilities or Limitations**: The person in the wheelchair could have a physical disability or limited mobility that prevents them from reaching their laptop directly.\\n\\n2. **Accessibility Barriers**: The physical setup of the room, including the height of the desk or the arrangement of objects, might be contributing to their difficulty.\\n\\n3. **Environmental Factors**: The office environment could be such that other elements like furniture, electronic equipment, or accessibility solutions are not properly positioned for ease of use.\\n\\n4. **Pain or Fatigue**: Physical discomfort could be causing the individual to avoid reaching out to their laptop.\\n\\n5. **Lack of Proper Training or Facilities**: If the workplace does not have adequate facilities or training for individuals with disabilities to use technology,']\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "IMAGE_PATH = \"./data/engineer_fitting_prosthetic_arm.jpg\"\n",
    "#\"work/data/engineer_fitting_prosthetic_arm.jpg\"\n",
    "im = Image.open(IMAGE_PATH)\n",
    "\n",
    "# Your code here\n",
    "from transformers import GenerationConfig\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "from transformers import TextStreamer\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You are a senior Prosthetist. Reply based on your experience and to the best of your knowledge.\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": im},        \n",
    "            {\"type\": \"text\", \"text\": \"Tell me: Based on your expertise, what could had happen to the person in this picture to reach that state?\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "temp = 1.0\n",
    "topK = 0\n",
    "topP = 0.9\n",
    "\n",
    "# Define a generation config\n",
    "gen_config = GenerationConfig(\n",
    "    max_new_tokens=200,   # limit reply length\n",
    "    temperature=temp,     # controls randomness (try 0.0, 0.5, 1.0, etc.)\n",
    "    top_k = topK,\n",
    "    do_sample=True,      # must be True for temperature to have effect\n",
    "    top_p=topP            # nucleus sampling (optional, helps variety)\n",
    ")\n",
    "    \n",
    "    \n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    conversation, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(conversation)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, generation_config=gen_config)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(\"\\nExpert reply:\", output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Retrieval Augmented Generation (RAG)\n",
    "\n",
    "#### Exercise 6\n",
    "\n",
    "Depending on the application of the project, you might need to extract text from given documents and include it as additional context. This becomes especially relevant if you have many documents that cannot possibly fit into the model's context window. To more easily implement a RAG pipeline we recommend the use of one of these libraries: [LangChain](https://python.langchain.com/v0.2/docs/introduction/), [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/), [Haystack](https://docs.haystack.deepset.ai/docs/intro).\n",
    "\n",
    "For the solution of this lab we will use *LangChain*.\n",
    "\n",
    "It can be useful to split this exercise into these steps:\n",
    "1. Read one or more documents using pdfminer\n",
    "2. Split the documents into small chunks\n",
    "3. Get and store the embeddings for each chunks\n",
    "5. Given a query, retrieve the most relevant chunk(s) and appropriately prompt your LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat4' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat7' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat4' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat7' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat4' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat7' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat4' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat7' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat4' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat7' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat4' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat7' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat4' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat7' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat4' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat7' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137050\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "DOC_PATH = \"./data/chain_of_thought_prompting.pdf\"\n",
    "text = extract_text(DOC_PATH)\n",
    "#print(text)\n",
    "print(len(text))\n",
    "\n",
    "# Suppose a user query\n",
    "#USER_QUERY = \"What is CoT?\"\n",
    "\n",
    "\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(DOC_PATH)\n",
    "data = loader.load()\n",
    "len(data)\n",
    "#print(data[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Split the documents into small chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "chunk_size = 512,\n",
    "chunk_overlap = 50,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(data)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get and store the embeddings for each chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = embedding_model.embed_documents(chunks[12].page_content)\n",
    "len(embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.07408566027879715,\n",
       " 0.009266193024814129,\n",
       " 0.03269228711724281,\n",
       " -0.006492134649306536,\n",
       " 0.009048717096447945,\n",
       " -0.06712459027767181,\n",
       " 0.17526036500930786,\n",
       " 0.03441877290606499,\n",
       " 0.015193158760666847,\n",
       " -0.038189973682165146]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Build FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FAISS index\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top results:\n",
      "\n",
      "Result 1:\n",
      "language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\n",
      "provided in the exemplars for few-shot prompting.\n",
      "Figure 1 shows an example of a model producing a chain of thought to solve a math word problem\n",
      "that it would have otherwise gotten incorrect. The chain...\n",
      "\n",
      "Result 2:\n",
      "Chain of thought after answer. Another potential beneﬁt of\n",
      "chain-of-thought prompting could simply be that such prompts\n",
      "allow the model to better access relevant knowledge acquired\n",
      "during pretraining. Therefore, we test an alternative conﬁgura-\n",
      "tion where the chain of thought prompt is only given af...\n",
      "\n",
      "Result 3:\n",
      "the sequential reasoning embodied in the chain of thought is\n",
      "useful for reasons beyond just activating knowledge.\n",
      "3.4 Robustness of Chain of Thought\n",
      "GSM8K\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20Solve rate (%)\n",
      "Standard prompting\n",
      "Chain-of-thought prompting\n",
      "·different annotator (B)\n",
      "·different annotator (C)\n",
      "·intentionally concise...\n"
     ]
    }
   ],
   "source": [
    "USER_QUERY = \"What is chain of thought?\"\n",
    "\n",
    "#Test similarity search\n",
    "#query = \"What is chain of thought prompting?\"\n",
    "results = vectorstore.similarity_search(USER_QUERY, k=3)\n",
    "\n",
    "print(\"\\nTop results:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\\n{doc.page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine retrieved chunks into one context string\n",
    "context = \"\\n\\n\".join([d.page_content for d in results])\n",
    "USER_QUERY = \"What is chain of thought?\"\n",
    "# Build prompt\n",
    "prompt = f\"Answer the following question based only on the context:\\n\\n{context}\\n\\nQuestion: {USER_QUERY}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d305fba6474302beac78d74e03651e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765e92fe26b6403a8ce3e409bf320da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ffd765ee5c4fa9a6031f726b540005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8b2122ec70438e838d81c8e0d5aa25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b7cabf3f11415f8f31c70ce55fec07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19aedb30a504bce86dec50573395c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a16c796f16d49e9afc7f27c6d8a78b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d877592eb841878250d51dea2df9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1de50f47de496a9ed6c29f97b0d913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d662ad258d40d19d18f89e2eece440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A chain of thought is a sequence of logical reasoning that a person uses to solve a problem or come to a conclusion. It is a sequence of thoughts that are connected to each other, leading to a conclusion. It is often used in problem-solving tasks, such as math problems, where the chain of thought is used to solve the problem.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, GenerationConfig\n",
    "\n",
    "# Define generation parameters\n",
    "gen_config = GenerationConfig(\n",
    "    temperature=0.7,   # randomness\n",
    "    top_k=50,          # consider top K candidates\n",
    "    top_p=0.9,         # nucleus sampling\n",
    "    max_new_tokens=256 # response length\n",
    ")\n",
    "\n",
    "\n",
    "# Load HuggingFace pipeline\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"tiiuae/falcon-7b-instruct\",  # pick a model that fits in your hardware\n",
    "    device_map=\"auto\"                   # uses GPU if available\n",
    ")\n",
    "\n",
    "# Generate response\n",
    "response = hf_pipeline(\n",
    "    prompt,\n",
    "    generation_config=gen_config,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create a user interface\n",
    "\n",
    "#### Exercise 7\n",
    "\n",
    "Since you are trying to build a complete application, you also need a nice user interface that interacts with the model. There are various libraries available for this purpose. Notably: [gradio](https://www.gradio.app/docs/gradio/interface) and [chat UI](https://huggingface.co/docs/chat-ui/index). For the solution of this lab, we will use gradio.\n",
    "\n",
    "Gradio has pre-defined input/output blocks that are automatically inserted in the interface. You only need to provide an appropriate function that takes all the inputs and returns the relevant output. See documentation [here](https://www.gradio.app/docs/gradio/interface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_similarity(query: str): \n",
    "    \n",
    "    #Test similarity search\n",
    "    #query = \"What is chain of thought prompting?\"\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    \n",
    "    return results[0].page_content[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# This part closes the demo server if it is already running (which\n",
    "# happens easily in notebooks) and prevents you from opening multiple\n",
    "# servers at the same time.\n",
    "if \"demo\" in locals() and demo.is_running:\n",
    "    demo.close()\n",
    "\n",
    "# Your code here\n",
    "#USER_QUERY = \"What is chain of thought?\"\n",
    "demo = gr.Interface(fn=function_similarity, inputs=\"textbox\", outputs=\"textbox\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
