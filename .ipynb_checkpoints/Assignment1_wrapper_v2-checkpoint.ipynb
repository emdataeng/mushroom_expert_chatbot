{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path 2 - HuggingFace\n",
    "HuggingFace (HF) is a free platform where user can upload models (of various kinds, not just LLMs) that can then be used through their `transformers` library. To be able to use the models on HF you don't need to create an account, however, some models are 'gated' and require approval from the creator before being able to use them (it is the case e.g. for LLaMA models). For those models, it's required both authentication and authorization to use the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. First simple generation\n",
    "For the means of this lab, we will use the model `Qwen/Qwen2.5-VL-3B-Instruct`, which is a non-gated fairly small model that, besides text, also support images and videos. For the assignment and the project you can choose the model that you prefer from the [HF catalogue](https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/accelerate/utils/modeling.py:1462: UserWarning: Current model requires 4776 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbbb6830e5b4f05b4a784272628f120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95eb4ea4c2384d41bfde5b95e1498d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0861867334334339a2494568d1aaea94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305306d79e8f434498f51db05027eb3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# fairly small but good model\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "# We're using the `Qwen2_5_VLForConditionalGeneration` class to enable multimodal generation\n",
    "# Normally, you can use AutoModelForCausalLM\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=\"auto\",  # automatically uses right precision based on model\n",
    "    device_map=\"auto\"  # automatically uses right device e.g. GPU if available\n",
    ")\n",
    "\n",
    "# We're using the `AutoProcessor` class to enable multimodal generation\n",
    "# Normally, you can use AutoTokenizer\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add images to the prompt\n",
    "This model, beside text also accepts images (and videos).\n",
    "\n",
    "\n",
    "#### Exercise 5\n",
    "Try prompting it with one. Choose an interesting image and prompt the model with a query about it.\n",
    "\n",
    "You can use the model's [README](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct).\n",
    "\n",
    "Use [PIL](https://pillow.readthedocs.io/en/stable/) to load an image. It should already be present in the Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function creates a text streamer to stream the output tokens as they are generated\n",
    "from transformers import TextStreamer\n",
    "def create_streamer():\n",
    "    return TextStreamer(\n",
    "        processor.tokenizer,\n",
    "        skip_prompt=True,  # do not print the prompt\n",
    "        skip_special_tokens=True  # do not print special tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function records in a log file the user prompt and the expert reply, as well as the generation parameters, the time taken to generate the reply and the model name, each entry is in json format. It also appends to the log file.\n",
    "import json\n",
    "def log_interaction(user_prompt, expert_reply, temp, topK, topP, time_taken, model_name=MODEL_NAME):\n",
    "    log_entry = {\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"expert_reply\": expert_reply,\n",
    "        \"temperature\": temp,\n",
    "        \"topK\": topK,\n",
    "        \"topP\": topP,\n",
    "        \"time_taken_seconds\": time_taken,\n",
    "        \"model_name\": model_name\n",
    "    }\n",
    "    with open(\"interaction_log.jsonl\", \"a\") as log_file:\n",
    "        log_file.write(json.dumps(log_entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expert reply: ['**Scientific Name:** Cantharellus cibarius (Champagne mushroom)\\n**Common Name:** Champagne mushroom\\n\\n**Family:** Cantharellaceae\\n\\n**Physical Description:**\\n- **Cap:** The cap is convex to bell-shaped, with a diameter of 3-7 cm. It is initially pale yellow to light brown, becoming darker with age.\\n- **Gills:** The gills are attached (decurrent), and they are initially white, becoming yellowish as the mushroom matures.\\n- **Stem:** The stem is equal to the cap, often with a slight depression at the base. It is smooth, yellowish to orange, and often has a ring at the top.\\n- **Spore Print:** The spore print is yellowish.\\n\\n**Habitat and Distribution:**\\n- They are commonly found in coniferous forests, particularly in Europe and North America.\\n- They are often found on the ground or in leaf litter.\\n\\n**Edibility:**\\n- **Edible:** Yes, the Champagne mushroom is considered a safe and delicious edible mushroom.\\n- **Symptoms:** None known.\\n- **Treatment:** None known.\\n\\n**Similarities:**\\n- They are similar to other cantharellus species, such as Cantharellus cibarius var. cibarius, which is the most common form of the Champagne mushroom.\\n\\n**Other Documented Facts:**\\n- The Champagne mushroom is a popular choice for gourmet mushroom enthusiasts and is often used in various culinary dishes.\\n- It is known for its earthy, slightly sweet flavor and is often paired with wild mushrooms in gourmet cooking.']\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import time\n",
    "IMAGE_PATH = \"./data/mushroom_copper_spike.jpg\"\n",
    "\n",
    "im = Image.open(IMAGE_PATH)\n",
    "\n",
    "# Your code here\n",
    "from transformers import GenerationConfig\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "\n",
    "\n",
    "user_prompt_str = \"Tell me both, the scientific and common names of the mushroom or mushrooms in this picture. Tell me the family to which they belong. Give me their physical description. Where are they most commonly found. Clarify if they are edible and if not, explain why and what are the sympthoms or side effects in a person, in this last case explain if there is a known treatment or antidote. Mention if there is any similarity with other specties. Lastly, you may mention any other documented fact about the mushroom.\"\n",
    "\n",
    "system_prompt_str = \"You are a mushroom expert chatbot that responds to user queries about mushrooms, and you must always steer the conversation to keep it in the context of mushrooms. You provide succinct and to the point information about mushrooms based on data. You talk in mycological terms about data. Also mention the common names of mushrooms. If you don't know the answer, just say you don't know. Never make up an answer.\"\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": system_prompt_str}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": im},        \n",
    "            {\"type\": \"text\", \"text\": user_prompt_str}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "temp = 0.9\n",
    "topK = 0\n",
    "topP = 0.5\n",
    "\n",
    "# Define a generation config\n",
    "gen_config = GenerationConfig(\n",
    "    max_new_tokens=500,   # limit reply length\n",
    "    temperature=temp,     # controls randomness (try 0.0, 0.5, 1.0, etc.)\n",
    "    top_k = topK,\n",
    "    do_sample=True,      # must be True for temperature to have effect\n",
    "    top_p=topP            # nucleus sampling (optional, helps variety)\n",
    ")\n",
    "    \n",
    "    \n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    conversation, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs, = process_vision_info(conversation)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "# Log the interaction with time taken\n",
    "start_time = time.time()\n",
    "# (Place the generation code here)\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, generation_config=gen_config)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\nExpert reply:\", output_text)\n",
    "\n",
    "time_taken = end_time - start_time\n",
    "log_interaction(user_prompt_str, output_text, temp, topK, topP, time_taken, model_name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create a user interface\n",
    "\n",
    "#### Exercise 7\n",
    "\n",
    "Since you are trying to build a complete application, you also need a nice user interface that interacts with the model. There are various libraries available for this purpose. Notably: [gradio](https://www.gradio.app/docs/gradio/interface) and [chat UI](https://huggingface.co/docs/chat-ui/index). For the solution of this lab, we will use gradio.\n",
    "\n",
    "Gradio has pre-defined input/output blocks that are automatically inserted in the interface. You only need to provide an appropriate function that takes all the inputs and returns the relevant output. See documentation [here](https://www.gradio.app/docs/gradio/interface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_similarity(query: str): \n",
    "    \n",
    "    #Test similarity search\n",
    "    #query = \"What is chain of thought prompting?\"\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    \n",
    "    return results[0].page_content[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# This part closes the demo server if it is already running (which\n",
    "# happens easily in notebooks) and prevents you from opening multiple\n",
    "# servers at the same time.\n",
    "if \"demo\" in locals() and demo.is_running:\n",
    "    demo.close()\n",
    "\n",
    "# Your code here\n",
    "#USER_QUERY = \"What is chain of thought?\"\n",
    "demo = gr.Interface(fn=function_similarity, inputs=\"textbox\", outputs=\"textbox\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
