{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path 2 - HuggingFace\n",
    "HuggingFace (HF) is a free platform where user can upload models (of various kinds, not just LLMs) that can then be used through their `transformers` library. To be able to use the models on HF you don't need to create an account, however, some models are 'gated' and require approval from the creator before being able to use them (it is the case e.g. for LLaMA models). For those models, it's required both authentication and authorization to use the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. First simple generation\n",
    "For the means of this lab, we will use the model `Qwen/Qwen2.5-VL-3B-Instruct`, which is a non-gated fairly small model that, besides text, also support images and videos. For the assignment and the project you can choose the model that you prefer from the [HF catalogue](https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/accelerate/utils/modeling.py:1462: UserWarning: Current model requires 4776 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87c5b8b8d884d28a5d4ebf8f860a457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d64c67531c345a390760e301f47b767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1b05e8348743ba953d2abd46ea5e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933093f8f9cf4a1f927e06fbd1f08df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# fairly small but good model\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "# We're using the `Qwen2_5_VLForConditionalGeneration` class to enable multimodal generation\n",
    "# Normally, you can use AutoModelForCausalLM\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=\"auto\",  # automatically uses right precision based on model\n",
    "    device_map=\"auto\"  # automatically uses right device e.g. GPU if available\n",
    ")\n",
    "\n",
    "# We're using the `AutoProcessor` class to enable multimodal generation\n",
    "# Normally, you can use AutoTokenizer\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add images to the prompt\n",
    "This model, beside text also accepts images (and videos).\n",
    "\n",
    "\n",
    "#### Exercise 5\n",
    "Try prompting it with one. Choose an interesting image and prompt the model with a query about it.\n",
    "\n",
    "You can use the model's [README](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct).\n",
    "\n",
    "Use [PIL](https://pillow.readthedocs.io/en/stable/) to load an image. It should already be present in the Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function creates a text streamer to stream the output tokens as they are generated\n",
    "from transformers import TextStreamer\n",
    "def create_streamer():\n",
    "    return TextStreamer(\n",
    "        processor.tokenizer,\n",
    "        skip_prompt=True,  # do not print the prompt\n",
    "        skip_special_tokens=True  # do not print special tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns timetaken of the interaction in string format DD-MM-YYYY_HH-MM-SS, the time taken must be in seconds if less than a minute, in minutes if less than an hour, in hours if less than a day and in days otherwise.\n",
    "def get_timestamp(time_taken):    \n",
    "    if time_taken < 60:\n",
    "        time_taken_str = f\"{time_taken} seconds\"\n",
    "    elif time_taken < 3600:\n",
    "        time_taken_str = f\"{time_taken // 60} minutes\"\n",
    "    elif time_taken < 86400:\n",
    "        time_taken_str = f\"{time_taken // 3600} hours\"\n",
    "    else:\n",
    "        time_taken_str = f\"{time_taken // 86400} days\"\n",
    "    return time_taken_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function records in a json log file the user prompt and the expert reply, as well as the generation parameters, the time taken to generate the reply and the model name. Each entry is created by appending to the file a a new key in the json object whose name is the timestamp of the interaction in format HH-MM-SS, the time taken variable must be in seconds if less than a minute, in minutes if less than an hour, in hours if less than a day and in days otherwise. The name of the log file is the current ime in format DD-MM-YYYY followed by _log.json. If the log file already exists, the new entry is appended to the existing file. If it does not exist, a new file is created.\n",
    "\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os \n",
    "def log_interaction(image_file_name, user_prompt, expert_reply, temp, topK, topP, time_taken, model_name, expert_reply_json=\"No Image\"):\n",
    "    # Get current date and time\n",
    "    now = datetime.now()\n",
    "    date_str = now.strftime(\"%d-%m-%Y\")\n",
    "    time_str = now.strftime(\"%H:%M:%S\")\n",
    "    \n",
    "    # Create log file name\n",
    "    log_file_name = f\"{date_str}_log.json\"\n",
    "    \n",
    "    # Create log entry\n",
    "    log_entry = {\n",
    "        \"image_file_name\": image_file_name,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"expert_reply_json\": expert_reply_json,\n",
    "        \"expert_reply\": expert_reply,\n",
    "        \"generation_parameters\": {\n",
    "            \"temperature\": temp,\n",
    "            \"topK\": topK,\n",
    "            \"topP\": topP\n",
    "        },\n",
    "        \"time_taken\": get_timestamp(time_taken),\n",
    "        \"model_name\": model_name\n",
    "    }\n",
    "    \n",
    "    # Load existing log file or create new one\n",
    "    if os.path.exists(log_file_name):\n",
    "        with open(log_file_name, \"r\") as f:\n",
    "            log_data = json.load(f)\n",
    "    else:\n",
    "        log_data = {}\n",
    "    \n",
    "    # Append new entry\n",
    "    log_data[time_str] = log_entry\n",
    "    \n",
    "    # Save updated log file\n",
    "    with open(log_file_name, \"w\") as f:\n",
    "        json.dump(log_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import time\n",
    "IMAGE_PATH = \"./data\"\n",
    "image_file_name = \"mushroom_copper_spike.jpg\"\n",
    "im = Image.open(f\"{IMAGE_PATH}/{image_file_name}\") #.convert(\"RGB\")??\n",
    "\n",
    "# Your code here\n",
    "from transformers import GenerationConfig\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "json_response_example = \"\"\"{\n",
    "    \"common_name\": \"Inkcap\",\n",
    "    \"genus\": \"Coprinus\",\n",
    "    \"confidence\": 0.5,\n",
    "    \"visible\": [\"cap\", \"hymenium\", \"stipe\"],\n",
    "    \"color\": \"orange\",\n",
    "    \"edible\": true\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "user_prompt_str = \"Tell me both, the scientific and common names of the mushroom or mushrooms in this picture. Tell me the family to which they belong. Give me their physical description. Where are they most commonly found. Clarify if they are edible and if not, explain why and what are the sympthoms or side effects in a person, in this last case explain if there is a known treatment or antidote. Mention if there is any similarity with other specties. Lastly, you may mention any other documented fact about the mushroom.\"\n",
    "\n",
    "system_prompt_str = f\"\"\"\n",
    "You are a mushroom expert chatbot. Your role: answer queries strictly about mushrooms using mycological terms, succinct and data-driven.\n",
    "\n",
    "Rules:\n",
    "- If you don’t know, say \"I don't know.\" Never invent facts.\n",
    "- Always include scientific and common names when known.\n",
    "- Keep answers concise and evidence-focused.\n",
    "\n",
    "Image handling & JSON (required):\n",
    "- If the user sends an image, first output exactly one valid JSON object with these fields:\n",
    "  - common_name (string or list)\n",
    "  - genus (string or list)\n",
    "  - family (string or list)\n",
    "  - confidence (float, 0–1)\n",
    "  - visible (list, choose from: [\"cap\",\"hymenium\",\"stipe\"])\n",
    "  - color (string)\n",
    "  - edible (bool)\n",
    "  - notes (optional short string for important facts)\n",
    "  Example: {json_response_example}\n",
    "- JSON must be the first output and strictly parsable (no extra text inside the JSON).\n",
    "\n",
    "After the JSON:\n",
    "- If the user asked a question when sending the image, answer it.\n",
    "- If no question, produce a one-paragraph summary of the image based on the JSON.\n",
    "\n",
    "Session state rules:\n",
    "- Persist the most recent image JSON for the duration of this chat.\n",
    "- For any follow-up question (e.g., \"Are these poisonous?\", \"What about the stem?\"), use the stored JSON as the factual basis unless:\n",
    "  - the user uploads a new image (replace stored JSON), or\n",
    "  - the user explicitly asks you to forget or replace the stored JSON.\n",
    "- If confidence < 0.35, state that identification is uncertain and avoid definitive claims.\n",
    "\n",
    "Formatting:\n",
    "- JSON first (parsable), then a single paragraph answer/summary.\n",
    "- Tone: professional, concise, mycological.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_json_and_explanation(output_text: str):\n",
    "    \"\"\"\n",
    "    Extract the JSON-like block and any explanation text from the model output.\n",
    "    \"\"\"\n",
    "    json_match = re.search(r\"```json(.*?)```\", output_text, re.DOTALL)\n",
    "\n",
    "    if json_match:\n",
    "        json_part = json_match.group(1).strip()\n",
    "        explanation = output_text[json_match.end():].strip()\n",
    "    else:\n",
    "        json_part = None\n",
    "        explanation = output_text.strip()\n",
    "\n",
    "    return json_part, explanation\n",
    "\n",
    "\n",
    "# def handle_model_response(output_text, image_name, user_prompt, params, runtime, model_name):\n",
    "#     \"\"\"\n",
    "#     Process the raw model output and return a structured log entry.\n",
    "#     \"\"\"\n",
    "#     json_block, explanation = extract_json_and_explanation(output_text)\n",
    "\n",
    "#     return {\n",
    "#         \"image_file_name\": image_name,\n",
    "#         \"user_prompt\": user_prompt,\n",
    "#         \"expert_reply_json\": json_block,\n",
    "#         \"expert_reply_explanation\": explanation,\n",
    "#         \"generation_parameters\": params,\n",
    "#         \"time_taken\": runtime,\n",
    "#         \"model_name\": model_name\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(user_prompt, image, temp, topK, topP, keep_in_mind=None) -> str:\n",
    "    \"\"\"\n",
    "    Wrapper function to call the LLM.\n",
    "    \"\"\"\n",
    "    # Conversation always starts with system prompt\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": system_prompt_str}\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Add persistent memory into system context\n",
    "    if keep_in_mind:\n",
    "        conversation.append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": f\"Keep in mind: {json.dumps(keep_in_mind)}\"}\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Add user message (with or without image)\n",
    "    user_content = []\n",
    "    if image is not None:\n",
    "        user_content.append({\"type\": \"image\", \"image\": image})\n",
    "    if user_prompt:\n",
    "        user_content.append({\"type\": \"text\", \"text\": user_prompt})\n",
    "\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_content})\n",
    "    \n",
    "    # Generation config\n",
    "    gen_config = GenerationConfig(\n",
    "        max_new_tokens=500,\n",
    "        temperature=temp,\n",
    "        top_k=topK,\n",
    "        do_sample=True,\n",
    "        top_p=topP\n",
    "    )\n",
    "    \n",
    "    # Prepare model inputs\n",
    "    text = processor.apply_chat_template(\n",
    "        conversation, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, video_inputs = process_vision_info(conversation)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    \n",
    "    \n",
    "    generated_ids = model.generate(**inputs, generation_config=gen_config)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep_in_mind with memoization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def chatbot_interface(user_prompt, image=None, temp=0.7, topK=0, topP=0.5, image_filename=None):\n",
    "    # Initialize memory (only first time)\n",
    "    if not hasattr(chatbot_interface, \"keep_in_mind\"):\n",
    "        chatbot_interface.keep_in_mind = {}\n",
    "\n",
    "    # Run generation\n",
    "    start_time = time.time()\n",
    "\n",
    "    raw_output_text = call_model(user_prompt, image, temp, topK, topP, chatbot_interface.keep_in_mind)\n",
    "    \n",
    "    end_time = time.time() \n",
    "    time_taken = end_time - start_time\n",
    "    \n",
    "    # Log interaction    \n",
    "    expert_reply_json,  expert_reply_explanation = extract_json_and_explanation(raw_output_text)\n",
    "\n",
    "    if expert_reply_json:\n",
    "        try:\n",
    "            # Update persistent memory\n",
    "            chatbot_interface.keep_in_mind = json.loads(expert_reply_json)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Warning: Failed to parse JSON image summary from model output.\")\n",
    "            expert_reply_json = \"Warning: Failed to parse JSON image summary from model output.\"\n",
    "            chatbot_interface.keep_in_mind = {}\n",
    "    elif not image and expert_reply_json is None: expert_reply_json = \"No image uploaded, so no explanation provided.\"\n",
    "            \n",
    "    print(\"Keep in mind:\", chatbot_interface.keep_in_mind)\n",
    "    print(\"expert_reply_json:\", expert_reply_json)        # The structured JSON block\n",
    "    print(\"expert_reply_explanation:\", expert_reply_explanation) # The natural-language explanation\n",
    "\n",
    "    \n",
    "    log_interaction(\n",
    "        f\"uploaded_image: {image_filename}\" if image else \"text_only\",\n",
    "        user_prompt,\n",
    "        expert_reply_explanation,\n",
    "        temp, topK, topP,\n",
    "        time_taken,\n",
    "        MODEL_NAME,\n",
    "        expert_reply_json\n",
    "    )\n",
    "    \n",
    "    # Return only the explanation (user-facing)\n",
    "    return expert_reply_explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50729/4136092332.py:35: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep in mind: {}\n",
      "No image uploaded, so no explanation provided.\n",
      "Yes, mushrooms can have a distinct smell. The smell of mushrooms is often used as a guide to their identification. Some mushrooms have a pleasant aroma, while others have a strong, pungent odor. The smell can vary depending on the species and the conditions under which the mushroom was growing.\n",
      "Keep in mind: {'common_name': ['Cuphophyllus'], 'genus': ['Cuphophyllus'], 'family': ['Hymenogastraceae'], 'confidence': 0.9, 'visible': ['cap', 'hymenium', 'stipe'], 'color': 'orange', 'edible': True, 'notes': 'This species is commonly found in forest floors and is edible.'}\n",
      "{\n",
      "    \"common_name\": [\"Cuphophyllus\"],\n",
      "    \"genus\": [\"Cuphophyllus\"],\n",
      "    \"family\": [\"Hymenogastraceae\"],\n",
      "    \"confidence\": 0.9,\n",
      "    \"visible\": [\"cap\", \"hymenium\", \"stipe\"],\n",
      "    \"color\": \"orange\",\n",
      "    \"edible\": true,\n",
      "    \"notes\": \"This species is commonly found in forest floors and is edible.\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Mushroom expert response function\n",
    "import gradio as gr\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "def response(image, question, history, image_filename):\n",
    "    if question and \"hello\" in question.lower():\n",
    "        return \"Hello! I am a mushroom expert. Ask me anything about mushrooms.\"\n",
    "    elif question and \"bye\" in question.lower():\n",
    "        return \"Goodbye! Have a great day.\"\n",
    "    elif image is not None: #and question.strip() != \"\"\n",
    "        reply = chatbot_interface(question, image, image_filename=image_filename)    \n",
    "        return reply\n",
    "        #return f\"I see you uploaded an image and asked: '{question}'. My guess: It's some kind of mushroom 🍄.\"\n",
    "    elif image is None and question.strip() != \"\":\n",
    "        reply = chatbot_interface(question, image)    \n",
    "        return reply\n",
    "    else:\n",
    "        return random.choice([\n",
    "            \"I am not sure about that. Can you ask me something else?\",\n",
    "            \"Could you reformulate your question? I am not sure I understand.\",\n",
    "            \"I don't understand, can you ask someone else?\",\n",
    "            \"What a stellar question! I am not sure about the answer though.\",\n",
    "            \"You know what? I am not cut out for this. I am going to take a break.\"\n",
    "        ])\n",
    "\n",
    "# Prevent multiple servers in Jupyter\n",
    "if \"demo\" in locals() and demo.is_running:\n",
    "    demo.close()\n",
    "\n",
    "with gr.Blocks(fill_height=True) as demo:\n",
    "    gr.Markdown(\"## 🍄 Your Personal Mushroom Expert 🍄‍🟫\")\n",
    "    gr.Markdown(\"Ask me anything about mushrooms! Upload a picture and type your question.\")\n",
    "    \n",
    "    chatbot = gr.Chatbot()      \n",
    "    \n",
    "    with gr.Row():\n",
    "        #image = gr.Image(type=\"pil\", label=\"Upload Mushroom Image\")\n",
    "        image = gr.Image(type=\"filepath\", label=\"Upload Mushroom Image\")        \n",
    "        text = gr.Textbox(label=\"Your Question\", placeholder=\"Ask me anything about mushrooms...\")\n",
    "    btn = gr.Button(\"Ask the Expert\")\n",
    "    \n",
    "    # Define the function to handle chat interactions, image it’s just a string path to the file\n",
    "    def chat_fn(image, text, history):\n",
    "        filename = None\n",
    "        pil_img = None\n",
    "\n",
    "        if image:\n",
    "            filename = os.path.basename(image) #keep filename for logging\n",
    "            pil_img = Image.open(image)  # convert path to PIL image\n",
    "\n",
    "        reply = response(pil_img, text, history, filename)\n",
    "        if text:\n",
    "            prev_prompt = text\n",
    "        elif pil_img is not None:\n",
    "            prev_prompt = f\"[Image only: {filename}]\"\n",
    "        else:\n",
    "            prev_prompt = \"\"\n",
    "        history = history + [(prev_prompt, reply)]\n",
    "        return history, None, None  # clear inputs\n",
    "    \n",
    "    btn.click(chat_fn, [image, text, chatbot], [chatbot, image, text])\n",
    "\n",
    "demo.launch(server_name=\"0.0.0.0\", server_port=7860)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User interface\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
